{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut8SGWcnxm_u"
      },
      "source": [
        "Gauname nuotraukų"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9MRum63xgRF",
        "outputId": "ce36cae3-ca75-41bf-b250-8a3864d37eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openimages in /usr/local/lib/python3.11/dist-packages (0.0.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from openimages) (1.37.0)\n",
            "Requirement already satisfied: cvdata in /usr/local/lib/python3.11/dist-packages (from openimages) (0.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from openimages) (5.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from openimages) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openimages) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openimages) (4.67.1)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.0 in /usr/local/lib/python3.11/dist-packages (from boto3->openimages) (1.37.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->openimages) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->openimages) (0.11.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->openimages) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openimages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Zzh469yXy0w0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openimages.download import download_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "q8pdi2Yby2pn"
      },
      "outputs": [],
      "source": [
        "data_dir = \"OpenImages\"\n",
        "number_for_samples = 333\n",
        "classes = [\"Strawberry\", \"Bee\", \"Goldfish\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QNghiy5LzX0C"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhmUpffgzuEI",
        "outputId": "ea2a576b-26f5-4f96-e37b-c7f5c2a31126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading is starting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 333/333 [00:22<00:00, 14.98it/s]\n",
            "100%|██████████| 333/333 [00:21<00:00, 15.50it/s]\n",
            "100%|██████████| 333/333 [00:21<00:00, 15.20it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'strawberry': {'images_dir': 'OpenImages/strawberry/images'},\n",
              " 'bee': {'images_dir': 'OpenImages/bee/images'},\n",
              " 'goldfish': {'images_dir': 'OpenImages/goldfish/images'}}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "print(\"Downloading is starting...\")\n",
        "download_dataset(data_dir, classes, limit=number_for_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtviX4rU62Iw"
      },
      "source": [
        "Paruošiam transformacijos parametrus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "TsqamwR_684z"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zam-Iy86j_0"
      },
      "source": [
        "Custom Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Y3HpAFbaAQwR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class GMMDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Custom dataset class to load images from 'data_dir/class_name/images/' structure.\n",
        "\n",
        "        Parameters:\n",
        "        - data_dir (str): Path to the dataset directory.\n",
        "        - transform (callable, optional): Transformations to apply to the images.\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Define class-to-index mapping (case insensitive)\n",
        "        self.class_to_idx = {\"strawberry\": 0, \"bee\": 1, \"goldfish\": 2}\n",
        "\n",
        "        # Scan the dataset structure correctly\n",
        "        for class_name, class_idx in self.class_to_idx.items():\n",
        "            class_path = os.path.join(data_dir, class_name, \"images\")  # Look inside 'images' subfolder\n",
        "            if os.path.exists(class_path):\n",
        "                for img_name in os.listdir(class_path):\n",
        "                    img_path = os.path.join(class_path, img_name)\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.labels.append(class_idx)\n",
        "\n",
        "        print(f\"Loaded {len(self.image_paths)} images from {data_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the number of images in the dataset.\"\"\"\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Loads an image and its label.\"\"\"\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOHoUpg51ck7"
      },
      "source": [
        "Create a dataset and put it in a dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zByi37zxClR5",
        "outputId": "4888f018-0b93-449e-9d28-c57aac6de0ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 999 images from OpenImages\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataset = GMMDataset(data_dir=\"OpenImages\", transform=transform)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers = 2)\n",
        "iterator = iter(dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr1RTVIHFoT8"
      },
      "source": [
        "Paruošiam pre-trained modelį darbui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8L5h4-UFt2U",
        "outputId": "554f3963-fa58-4722-8ce4-0800ad4bd1bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (31): ReLU(inplace=True)\n",
              "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (33): ReLU(inplace=True)\n",
              "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (35): ReLU(inplace=True)\n",
              "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = models.vgg19(pretrained=True)\n",
        "model.eval().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUCF51ghXs5z"
      },
      "source": [
        "Skaičiavimai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIhMmmaCX2Pv",
        "outputId": "017c5492-37ef-4f1e-c736-b32fedf81b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class Strawberry Metrics:\n",
            "  Accuracy : nan\n",
            "  Recall   : 0.00\n",
            "  Precision: 0.00\n",
            "  F1-score : 0.00\n",
            "\n",
            "Class Bee Metrics:\n",
            "  Accuracy : nan\n",
            "  Recall   : 0.00\n",
            "  Precision: 0.00\n",
            "  F1-score : 0.00\n",
            "\n",
            "Class Goldfish Metrics:\n",
            "  Accuracy : nan\n",
            "  Recall   : 0.00\n",
            "  Precision: 0.00\n",
            "  F1-score : 0.00\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "\n",
        "# Define selected class indices\n",
        "STRAWBERRY_IDX = 949  # Strawberry\n",
        "BEE_IDX = 309  # Bee\n",
        "GOLDFISH_IDX = 1  # Zebra\n",
        "\n",
        "ground_truth = []\n",
        "predictions_strawberry = []\n",
        "predictions_bee = []\n",
        "predictions_goldfish = []\n",
        "\n",
        "# Perform inference\n",
        "while True:\n",
        "    try:\n",
        "        features, labels = next(iterator)\n",
        "        output = model(features.to(device))\n",
        "\n",
        "        for i in range(output.shape[0]):\n",
        "            predictions = torch.softmax(output[i], 0)\n",
        "            predictions_strawberry = np.append(predictions_strawberry, predictions[STRAWBERRY_IDX].cpu().detach())\n",
        "            predictions_bee = np.append(predictions_bee, predictions[BEE_IDX].cpu().detach())\n",
        "            predictions_goldfish = np.append(predictions_goldfish, predictions[GOLDFISH_IDX].cpu().detach())\n",
        "\n",
        "        ground_truth = np.append(ground_truth, labels)\n",
        "    except StopIteration:\n",
        "        break\n",
        "\n",
        "# Compute confusion matrix\n",
        "def calculate_confusion_matrix(ground_truth, predictions, class_idx, threshold=0.5):\n",
        "    predictions = np.array(predictions)  # Convert to NumPy array\n",
        "    predictions_thresholded = (predictions >= threshold).astype(np.float64)\n",
        "\n",
        "    matrix = {\n",
        "        'TP': np.sum((ground_truth == class_idx) & (predictions_thresholded == 1)),\n",
        "        'TN': np.sum((ground_truth != class_idx) & (predictions_thresholded == 0)),\n",
        "        'FP': np.sum((ground_truth != class_idx) & (predictions_thresholded == 1)),\n",
        "        'FN': np.sum((ground_truth == class_idx) & (predictions_thresholded == 0)),\n",
        "    }\n",
        "    return matrix\n",
        "\n",
        "\n",
        "# Compute metrics\n",
        "def calculate_metrics(TP, TN, FP, FN):\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = (TP + TN) / (TP + FP + TN + FN)\n",
        "    metrics['recall'] = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "    metrics['precision'] = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "    metrics['f1'] = 2 * (metrics['recall'] * metrics['precision']) / (metrics['recall'] + metrics['precision']) if (metrics['recall'] + metrics['precision']) else 0\n",
        "    return metrics\n",
        "\n",
        "# Evaluate for each class\n",
        "matrix_strawberry = calculate_confusion_matrix(ground_truth, predictions_strawberry, 0, 0.05)\n",
        "matrix_bee = calculate_confusion_matrix(ground_truth, predictions_bee, 1, 0.05)\n",
        "matrix_goldfish = calculate_confusion_matrix(ground_truth, predictions_goldfish, 2, 0.05)\n",
        "\n",
        "metrics_strawberry = calculate_metrics(matrix_strawberry['TP'], matrix_strawberry['TN'], matrix_strawberry['FP'], matrix_strawberry['FN'])\n",
        "metrics_bee = calculate_metrics(matrix_bee['TP'], matrix_bee['TN'], matrix_bee['FP'], matrix_bee['FN'])\n",
        "metrics_goldfish = calculate_metrics(matrix_goldfish['TP'], matrix_goldfish['TN'], matrix_goldfish['FP'], matrix_goldfish['FN'])\n",
        "\n",
        "# Print results\n",
        "def print_metrics(metrics, class_name):\n",
        "    print(f'Class {class_name} Metrics:')\n",
        "    print(f'  Accuracy : {metrics[\"accuracy\"]:.2f}')\n",
        "    print(f'  Recall   : {metrics[\"recall\"]:.2f}')\n",
        "    print(f'  Precision: {metrics[\"precision\"]:.2f}')\n",
        "    print(f'  F1-score : {metrics[\"f1\"]:.2f}\\n')\n",
        "\n",
        "\n",
        "print_metrics(metrics_strawberry, \"Strawberry\")\n",
        "print_metrics(metrics_bee, \"Bee\")\n",
        "print_metrics(metrics_goldfish, \"Goldfish\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import shutil # Istrinam aplankus\n",
        "\n",
        "shutil.rmtree('OpenImages')\n",
        "shutil.rmtree('OpenImages_split')"
      ],
      "metadata": {
        "id": "pV3ZFvOK_WiS"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openimages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwkdHPn-keyF",
        "outputId": "5e690cad-f947-4ed0-8416-3311fd75ff83"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openimages in /usr/local/lib/python3.11/dist-packages (0.0.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from openimages) (1.37.28)\n",
            "Requirement already satisfied: cvdata in /usr/local/lib/python3.11/dist-packages (from openimages) (0.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from openimages) (5.3.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from openimages) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from openimages) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openimages) (4.67.1)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.28 in /usr/local/lib/python3.11/dist-packages (from boto3->openimages) (1.37.28)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->openimages) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->openimages) (0.11.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from cvdata->openimages) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->openimages) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->openimages) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->openimages) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from openimages.download import download_dataset\n",
        "\n",
        "\n",
        "data_dir = \"OpenImages\" # Nurodome direktorija\n",
        "number_for_samples = 333\n",
        "classes = [\"Strawberry\", \"Bee\", \"Goldfish\"] # Pasirinktos klases is OpenImages\n",
        "\n",
        "\n",
        "if not os.path.exists(data_dir):\n",
        "    os.makedirs(data_dir)\n",
        "\n",
        "\n",
        "print(\"Downloading is starting...\")\n",
        "download_dataset(data_dir, classes, limit=number_for_samples) #Atsisiunciame nuotraukas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2vsvVM8likV",
        "outputId": "4a1ca892-5be7-479b-e1e7-7629b55f0b6b"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading is starting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 333/333 [00:19<00:00, 16.90it/s]\n",
            "100%|██████████| 333/333 [00:21<00:00, 15.63it/s]\n",
            "100%|██████████| 333/333 [00:19<00:00, 16.94it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'strawberry': {'images_dir': 'OpenImages/strawberry/images'},\n",
              " 'bee': {'images_dir': 'OpenImages/bee/images'},\n",
              " 'goldfish': {'images_dir': 'OpenImages/goldfish/images'}}"
            ]
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import shutil\n",
        "\n",
        "def split_dataset(base_dir, split_ratios=(0.7, 0.2, 0.1)):\n",
        "    assert abs(sum(split_ratios) - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
        "    sets = ['train', 'validate', 'test']\n",
        "\n",
        "    for cls in os.listdir(base_dir):\n",
        "        class_dir = os.path.join(base_dir, cls, \"images\")\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        images = [f for f in os.listdir(class_dir) if f.endswith('.jpg')]\n",
        "        if not images:\n",
        "            print(f\"No images found for class: {cls}\")\n",
        "            continue\n",
        "\n",
        "        random.shuffle(images)\n",
        "\n",
        "        total = len(images)\n",
        "        train_end = int(split_ratios[0] * total)\n",
        "        val_end = train_end + int(split_ratios[1] * total)\n",
        "\n",
        "        splits = {\n",
        "            'train': images[:train_end],\n",
        "            'validate': images[train_end:val_end],\n",
        "            'test': images[val_end:]\n",
        "        }\n",
        "\n",
        "        for split_name in sets:\n",
        "            split_dir = os.path.join(base_dir + \"_split\", split_name, cls)\n",
        "            os.makedirs(split_dir, exist_ok=True)\n",
        "            for img in splits[split_name]:\n",
        "                src = os.path.join(class_dir, img)\n",
        "                dst = os.path.join(split_dir, img)\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "    print(\"Dataset successfully split into train, validate, and test!\")\n",
        "\n",
        "split_dataset(data_dir)"
      ],
      "metadata": {
        "id": "MV4CIkf0-xH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf6d8113-2322-46ac-b83d-48a890f36e49"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully split into train, validate, and test!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "FdcTvaYohoEi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "import timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Lab2Dataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data = ImageFolder(data_dir, transform)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    @property\n",
        "    def classes(self):\n",
        "        return self.data.classes"
      ],
      "metadata": {
        "id": "mGuy8q1ljXN3"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((128, 128)),  # Resize first → faster aug\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),  # Smaller angle\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((128, 128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "    ])\n",
        "}"
      ],
      "metadata": {
        "id": "YPkAhjmiuO_Y"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Lab2Dataset(data_dir='OpenImages_split/train', transform = transform['train'])\n",
        "val_dataset = Lab2Dataset(data_dir='OpenImages_split/validate', transform = transform['validation'])\n",
        "test_dataset = Lab2Dataset(data_dir='OpenImages_split/test', transform = transform['test'])"
      ],
      "metadata": {
        "id": "3uLlh_fToPeg"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "w36JSmVluXCi"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Lab2CNN(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(Lab2CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 16 * 16, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout1(F.leaky_relu(self.fc1(x)))\n",
        "        x = self.dropout2(F.leaky_relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "XRRYfLhIx85n"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Lab2CNN(num_classes=3)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "mpL9dvgJ0xYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb8c2c91-7fd6-4e42-eab8-77b19735ccfb"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Lab2CNN(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=32768, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=3, bias=True)\n",
              "  (dropout1): Dropout(p=0.3, inplace=False)\n",
              "  (dropout2): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "dRfsRU7u1SZE"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 30\n",
        "train_losses, val_losses = [], []"
      ],
      "metadata": {
        "id": "5YcIqoT05JFf"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "    train_loss = running_loss / len(train_dataloader.dataset)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "    val_loss = running_loss / len(val_dataloader.dataset)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epoch} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Qoc9TUEV5WpA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfa7df1-2702-4b1b-b1df-854c9feb10be"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30 - Train Loss: 3.5999 - Val Loss: 0.9511\n",
            "Epoch 2/30 - Train Loss: 1.0915 - Val Loss: 0.8957\n",
            "Epoch 3/30 - Train Loss: 0.8811 - Val Loss: 0.7865\n",
            "Epoch 4/30 - Train Loss: 0.8226 - Val Loss: 0.7415\n",
            "Epoch 5/30 - Train Loss: 0.7104 - Val Loss: 0.6862\n",
            "Epoch 6/30 - Train Loss: 0.7290 - Val Loss: 0.6642\n",
            "Epoch 7/30 - Train Loss: 0.7136 - Val Loss: 0.6160\n",
            "Epoch 8/30 - Train Loss: 0.6963 - Val Loss: 0.6689\n",
            "Epoch 9/30 - Train Loss: 0.6182 - Val Loss: 0.6829\n",
            "Epoch 10/30 - Train Loss: 0.5936 - Val Loss: 0.6676\n",
            "Epoch 11/30 - Train Loss: 0.6733 - Val Loss: 0.5462\n",
            "Epoch 12/30 - Train Loss: 0.5790 - Val Loss: 0.5098\n",
            "Epoch 13/30 - Train Loss: 0.5421 - Val Loss: 0.4899\n",
            "Epoch 14/30 - Train Loss: 0.5523 - Val Loss: 0.4609\n",
            "Epoch 15/30 - Train Loss: 0.4767 - Val Loss: 0.6216\n",
            "Epoch 16/30 - Train Loss: 0.5051 - Val Loss: 0.4825\n",
            "Epoch 17/30 - Train Loss: 0.4635 - Val Loss: 0.6148\n",
            "Epoch 18/30 - Train Loss: 0.3883 - Val Loss: 0.8437\n",
            "Epoch 19/30 - Train Loss: 0.4260 - Val Loss: 0.5252\n",
            "Epoch 20/30 - Train Loss: 0.3935 - Val Loss: 0.6574\n",
            "Epoch 21/30 - Train Loss: 0.4364 - Val Loss: 0.5345\n",
            "Epoch 22/30 - Train Loss: 0.3790 - Val Loss: 0.3972\n",
            "Epoch 23/30 - Train Loss: 0.3619 - Val Loss: 0.5661\n",
            "Epoch 24/30 - Train Loss: 0.3347 - Val Loss: 0.4212\n",
            "Epoch 25/30 - Train Loss: 0.3616 - Val Loss: 0.8843\n",
            "Epoch 26/30 - Train Loss: 0.3544 - Val Loss: 0.5004\n",
            "Epoch 27/30 - Train Loss: 0.3070 - Val Loss: 0.5389\n",
            "Epoch 28/30 - Train Loss: 0.3465 - Val Loss: 0.5593\n",
            "Epoch 29/30 - Train Loss: 0.3519 - Val Loss: 0.5036\n",
            "Epoch 30/30 - Train Loss: 0.3334 - Val Loss: 0.5081\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "class_labels = test_dataset.classes\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precisions = precision_score(y_true, y_pred, average=None)\n",
        "recalls = recall_score(y_true, y_pred, average=None)\n",
        "f1s = f1_score(y_true, y_pred, average=None)\n",
        "\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(f\"\\nTikslumas (Accuracy): {accuracy:.4f}\\n\")\n",
        "\n",
        "print(\"Rezultatai pagal klases:\")\n",
        "for i, label in enumerate(class_labels):\n",
        "    print(f\"{label}\")\n",
        "    print(f\"  Precizija (Precision): {precisions[i]:.4f}\")\n",
        "    print(f\"  Atkūrimas (Recall):    {recalls[i]:.4f}\")\n",
        "    print(f\"  F1 reikšmė:            {f1s[i]:.4f}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34by7y_JOjyC",
        "outputId": "1a7aa099-f80b-4787-d379-385f508738e4"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[27  5  2]\n",
            " [ 1 33  0]\n",
            " [ 0  6 28]]\n",
            "\n",
            "Tikslumas (Accuracy): 0.8627\n",
            "\n",
            "Rezultatai pagal klases:\n",
            "bee\n",
            "  Precizija (Precision): 0.9643\n",
            "  Atkūrimas (Recall):    0.7941\n",
            "  F1 reikšmė:            0.8710\n",
            "\n",
            "goldfish\n",
            "  Precizija (Precision): 0.7500\n",
            "  Atkūrimas (Recall):    0.9706\n",
            "  F1 reikšmė:            0.8462\n",
            "\n",
            "strawberry\n",
            "  Precizija (Precision): 0.9333\n",
            "  Atkūrimas (Recall):    0.8235\n",
            "  F1 reikšmė:            0.8750\n",
            "\n"
          ]
        }
      ]
    }
  ]
}